{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e169595-883f-4367-abb1-709d61f4e88b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n",
      "Some weights of the model checkpoint at joeddav/xlm-roberta-large-xnli were not used when initializing XLMRobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmenting dataset with emotion labels...\n",
      "Detecting emotions for English texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|██████████████████████████████████████████████████████████████| 500/500 [12:23<00:00,  1.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detecting emotions for Hindi texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing texts: 100%|████████████████████████████████████████████████████████████| 500/500 [1:42:13<00:00, 12.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing and saving the final dataset...\n",
      "Process completed. Augmented dataset saved to: augmented_translation_data.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "from tqdm import tqdm  # For progress tracking\n",
    "\n",
    "# Suppress Symlink Warning\n",
    "os.environ[\"HF_HUB_DISABLE_SYMLINKS_WARNING\"] = \"1\"\n",
    "\n",
    "# Load classification pipelines\n",
    "english_emotion_pipeline = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"joeddav/xlm-roberta-large-xnli\"\n",
    ")\n",
    "\n",
    "hindi_emotion_pipeline = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    model=\"joeddav/xlm-roberta-large-xnli\"\n",
    ")\n",
    "\n",
    "# Function to preprocess text (remove special characters, excessive whitespace)\n",
    "def preprocess_text(text, max_tokens=512, drop_long_texts=False):\n",
    "    text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in text)  # Remove special symbols\n",
    "    text = ' '.join(text.split())  # Remove extra spaces\n",
    "\n",
    "    tokenized_text = text.split()\n",
    "    if len(tokenized_text) > max_tokens:\n",
    "        if drop_long_texts:\n",
    "            return None\n",
    "        text = ' '.join(tokenized_text[:max_tokens])  # Truncate text\n",
    "\n",
    "    return text.strip() if text.strip() else None\n",
    "\n",
    "\n",
    "# Function to detect emotions in a batch\n",
    "def detect_emotions_batch(texts, pipeline_func, max_tokens=512):\n",
    "    labels = [\"joy\", \"anger\", \"sadness\", \"fear\", \"love\", \"surprise\"]  # Define emotion labels\n",
    "    results = []\n",
    "    for text in tqdm(texts, desc=\"Processing texts\"):\n",
    "        preprocessed_text = preprocess_text(text, max_tokens=max_tokens, drop_long_texts=True)\n",
    "        if preprocessed_text is None:\n",
    "            results.append((\"DROPPED\", 0.0))\n",
    "            continue\n",
    "        try:\n",
    "            emotion_result = pipeline_func(preprocessed_text, candidate_labels=labels)\n",
    "            dominant_emotion = max(zip(emotion_result['labels'], emotion_result['scores']), key=lambda x: x[1])\n",
    "            results.append(dominant_emotion)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing text: '{text[:50]}...' -> {e}\")\n",
    "            results.append((\"UNKNOWN\", 0.0))  # Fallback for errors\n",
    "    return results\n",
    "\n",
    "\n",
    "# Function to augment dataset with emotion labels for both English and Hindi\n",
    "def augment_dataset_with_emotions(file_path, sample_size=None, max_tokens=512):\n",
    "    data = pd.read_csv(file_path)\n",
    "\n",
    "    if sample_size:\n",
    "        data = data.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    data['english'] = data['english'].astype(str).fillna(\"\")\n",
    "    data['hindi'] = data['hindi'].astype(str).fillna(\"\")\n",
    "\n",
    "    # Detect emotions for English and Hindi with progress tracking\n",
    "    print(\"Detecting emotions for English texts...\")\n",
    "    english_results = detect_emotions_batch(data['english'].tolist(), english_emotion_pipeline, max_tokens=max_tokens)\n",
    "    \n",
    "    print(\"Detecting emotions for Hindi texts...\")\n",
    "    hindi_results = detect_emotions_batch(data['hindi'].tolist(), hindi_emotion_pipeline, max_tokens=max_tokens)\n",
    "\n",
    "    # Extract emotions and confidences\n",
    "    english_emotions, english_confidences = zip(*english_results)\n",
    "    hindi_emotions, hindi_confidences = zip(*hindi_results)\n",
    "\n",
    "    # Add new columns\n",
    "    data['English_Emotion'] = english_emotions\n",
    "    data['English_Confidence'] = english_confidences\n",
    "    data['Hindi_Emotion'] = hindi_emotions\n",
    "    data['Hindi_Confidence'] = hindi_confidences\n",
    "\n",
    "    # Drop rows where either emotion was marked as \"DROPPED\"\n",
    "    data = data[(data['English_Emotion'] != \"DROPPED\") & (data['Hindi_Emotion'] != \"DROPPED\")]\n",
    "    return data\n",
    "\n",
    "\n",
    "# Function to prepare translation data\n",
    "def prepare_translation_data(data):\n",
    "    return [\n",
    "        (row['english'], row['hindi'], row['English_Emotion'], row['English_Confidence'], row['Hindi_Emotion'], row['Hindi_Confidence'])\n",
    "        for _, row in data.iterrows()\n",
    "    ]\n",
    "\n",
    "\n",
    "# Function to save the processed data\n",
    "def save_training_data(data, output_path):\n",
    "    df = pd.DataFrame(\n",
    "        data,\n",
    "        columns=['english', 'hindi', 'English_Emotion', 'English_Confidence', 'Hindi_Emotion', 'Hindi_Confidence']\n",
    "    )\n",
    "    df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# File paths\n",
    "file_path = 'hindi_english_parallel.csv'  # Input file path\n",
    "output_path = 'augmented_translation_data.csv'  # Output file path\n",
    "\n",
    "# Use a smaller dataset for testing or full dataset\n",
    "print(\"Augmenting dataset with emotion labels...\")\n",
    "augmented_data = augment_dataset_with_emotions(file_path, sample_size=500, max_tokens=512)\n",
    "\n",
    "# Prepare and save the processed data\n",
    "print(\"Preparing and saving the final dataset...\")\n",
    "final_data = prepare_translation_data(augmented_data)\n",
    "save_training_data(final_data, output_path)\n",
    "\n",
    "print(\"Process completed. Augmented dataset saved to:\", output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e48335e-22c0-4d5e-b9cf-3cadfad7fa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 7.4153\n",
      "Epoch 2/10, Loss: 6.7807\n",
      "Epoch 3/10, Loss: 6.6062\n",
      "Epoch 4/10, Loss: 6.4686\n",
      "Epoch 5/10, Loss: 6.3791\n",
      "Epoch 6/10, Loss: 6.3083\n",
      "Epoch 7/10, Loss: 6.2320\n",
      "Epoch 8/10, Loss: 6.1721\n",
      "Epoch 9/10, Loss: 6.1138\n",
      "Epoch 10/10, Loss: 6.0532\n",
      "Model training complete and saved.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# Load dataset\n",
    "data_path = 'augmented_translation_data.csv'\n",
    "df = pd.read_csv(data_path).dropna().sample(frac=1).reset_index(drop=True)\n",
    "df = df[['english', 'hindi']]\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "# Vocabulary Building\n",
    "def build_vocab(texts):\n",
    "    vocab = {\"<unk>\": 0, \"<pad>\": 1, \"<sos>\": 2, \"<eos>\": 3}\n",
    "    for text in texts:\n",
    "        for token in tokenize(text):\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "src_vocab = build_vocab(df['english'])\n",
    "trg_vocab = build_vocab(df['hindi'])\n",
    "\n",
    "# Save vocabularies\n",
    "torch.save(src_vocab, \"src_vocab.pth\")\n",
    "torch.save(trg_vocab, \"trg_vocab.pth\")\n",
    "\n",
    "# Custom Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data, src_vocab, trg_vocab):\n",
    "        self.data = data\n",
    "        self.src_vocab = src_vocab\n",
    "        self.trg_vocab = trg_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = tokenize(self.data.iloc[idx]['english'])\n",
    "        trg = tokenize(self.data.iloc[idx]['hindi'])\n",
    "\n",
    "        src_tensor = [self.src_vocab.get(token, 0) for token in [\"<sos>\"] + src + [\"<eos>\"]]\n",
    "        trg_tensor = [self.trg_vocab.get(token, 0) for token in [\"<sos>\"] + trg + [\"<eos>\"]]\n",
    "\n",
    "        return torch.tensor(src_tensor), torch.tensor(trg_tensor)\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "EMBED_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 10\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# DataLoader\n",
    "def collate_fn(batch):\n",
    "    src_batch, trg_batch = zip(*batch)\n",
    "    src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=1, batch_first=False)\n",
    "    trg_batch = torch.nn.utils.rnn.pad_sequence(trg_batch, padding_value=1, batch_first=False)\n",
    "    return src_batch, trg_batch\n",
    "\n",
    "train_dataset = TranslationDataset(df, src_vocab, trg_vocab)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Encoder\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=1)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "# Decoder\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=1)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(0)\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        trg_len, batch_size = trg.shape\n",
    "        trg_vocab_size = self.decoder.fc_out.out_features\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[t] if random.random() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "# Model Setup\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(trg_vocab)\n",
    "encoder = Encoder(INPUT_DIM, EMBED_DIM, HID_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBED_DIM, HID_DIM, N_LAYERS, DROPOUT).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "# Loss & Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=1)\n",
    "\n",
    "# Training Function\n",
    "def train(model, loader, optimizer, criterion, device, clip=1.0):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, trg in loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].reshape(-1, output_dim)\n",
    "        trg = trg[1:].reshape(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(loader)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loss = train(model, train_loader, optimizer, criterion, DEVICE)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Save Model\n",
    "torch.save(model.state_dict(), \"translation_model.pth\")\n",
    "print(\"Model training complete and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "635e99f2-7e01-4239-a393-34ecf127296e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRANSLATION EVALUATION RESULTS =====\n",
      "🔹 Average BLEU Score: 0.1969\n",
      "🔹 Average ROUGE-1 Score: 0.3815\n",
      "🔹 Average ROUGE-2 Score: 0.2190\n",
      "🔹 Average ROUGE-L Score: 0.3650\n",
      "🔹 Emotion Accuracy: 83.70%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Data\n",
    "translated_data_path = 'translated_with_emotion_evaluation.csv'\n",
    "data = pd.read_csv(translated_data_path).dropna(subset=[\"Original_Hindi\", \"Translated_Text\"])  # Ensure no missing values\n",
    "\n",
    "# Initialize Metrics\n",
    "rouge = Rouge()\n",
    "smoothie = SmoothingFunction().method4  # BLEU smoothing function\n",
    "\n",
    "# Function to Compute BLEU Score\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str) or not reference or not hypothesis:\n",
    "        return 0.0  # Handle empty strings safely\n",
    "    \n",
    "    reference_tokens = [reference.split()]\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    \n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "# Function to Compute ROUGE Score\n",
    "def compute_rouge(reference, hypothesis):\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str) or not reference or not hypothesis:\n",
    "        return {\"rouge-1\": 0.0, \"rouge-2\": 0.0, \"rouge-l\": 0.0}\n",
    "    \n",
    "    scores = rouge.get_scores(hypothesis, reference)[0]  # Get ROUGE scores\n",
    "    return {\n",
    "        \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"rouge-l\": scores[\"rouge-l\"][\"f\"]\n",
    "    }\n",
    "\n",
    "# Function to Evaluate Emotion Accuracy\n",
    "def compute_emotion_accuracy(data):\n",
    "    actual_emotions = data[\"Input_Emotion\"].tolist()\n",
    "    translated_emotions = data[\"Translated_Emotion\"].tolist()\n",
    "    \n",
    "    # Filter out cases where the translated emotion is \"UNKNOWN\"\n",
    "    valid_indices = [i for i in range(len(actual_emotions)) if translated_emotions[i] != \"UNKNOWN\"]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    actual_filtered = [actual_emotions[i] for i in valid_indices]\n",
    "    translated_filtered = [translated_emotions[i] for i in valid_indices]\n",
    "\n",
    "    return accuracy_score(actual_filtered, translated_filtered)\n",
    "\n",
    "# Compute Scores for Each Row\n",
    "bleu_scores = []\n",
    "rouge1_scores, rouge2_scores, rougel_scores = [], [], []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    reference = row[\"Original_Hindi\"]\n",
    "    hypothesis = row[\"Translated_Text\"]\n",
    "\n",
    "    bleu_scores.append(compute_bleu(reference, hypothesis))\n",
    "    rouge_scores = compute_rouge(reference, hypothesis)\n",
    "    \n",
    "    rouge1_scores.append(rouge_scores[\"rouge-1\"])\n",
    "    rouge2_scores.append(rouge_scores[\"rouge-2\"])\n",
    "    rougel_scores.append(rouge_scores[\"rouge-l\"])\n",
    "\n",
    "# Compute Averages\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "avg_rouge1 = np.mean(rouge1_scores)\n",
    "avg_rouge2 = np.mean(rouge2_scores)\n",
    "avg_rougel = np.mean(rougel_scores)\n",
    "emotion_accuracy = compute_emotion_accuracy(data)\n",
    "\n",
    "# Display Evaluation Results\n",
    "print(\"\\n===== TRANSLATION EVALUATION RESULTS =====\")\n",
    "print(f\"🔹 Average BLEU Score: {avg_bleu:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-2 Score: {avg_rouge2:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-L Score: {avg_rougel:.4f}\")\n",
    "print(f\"🔹 Emotion Accuracy: {emotion_accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e0b58c-03d8-4264-b67b-b77a96746573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns: ['Original_English', 'Original_Hindi', 'Translated_Text', 'Input_Emotion', 'Translated_Emotion', 'Confidence_Deviation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Translating: 100%|██████████| 497/497 [26:30<00:00,  3.20s/sentence] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Translation completed! Saved results to nllb_translated_output.csv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "from tqdm import tqdm  # Import tqdm for progress tracking\n",
    "\n",
    "# Load NLLB-200 Model (Facebook's Translation Model)\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Function to translate English to Hindi\n",
    "def translate_sentence(text, src_lang=\"eng_Latn\", tgt_lang=\"hin_Deva\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang) \n",
    "    translated_tokens = model.generate(**inputs)\n",
    "    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Load Data\n",
    "data_path = \"translated_with_emotion_evaluation.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check column names\n",
    "print(\"CSV Columns:\", data.columns.tolist())\n",
    "\n",
    "# Ensure correct column names\n",
    "expected_columns = [\"Original_Hindi\", \"Original_English\"]\n",
    "data = data.dropna(subset=expected_columns)\n",
    "\n",
    "# Translate all English sentences with a progress bar\n",
    "translated_texts = []\n",
    "for _, row in tqdm(data.iterrows(), total=len(data), desc=\"Translating\", unit=\"sentence\"):\n",
    "    english_text = row[\"Original_English\"]\n",
    "    translated_text = translate_sentence(english_text)\n",
    "    translated_texts.append(translated_text)\n",
    "\n",
    "# Add new translations to the DataFrame\n",
    "data[\"Translated_Text\"] = translated_texts\n",
    "\n",
    "# Save to a new file\n",
    "new_file_path = \"nllb_translated_output.csv\"\n",
    "data.to_csv(new_file_path, index=False)\n",
    "\n",
    "print(f\"✅ Translation completed! Saved results to {new_file_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5023c43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 English-to-Hindi Translator (Type 'exit' to quit) 🔹\n",
      "📝 Translated: मैं तुमसे नफरत करता हूँ\n",
      "📝 Translated: मैं आप की प्रशंसा करता हूँ\n",
      "👋 Exiting translator. Have a great day!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "# Load NLLB-200 Model (Facebook's Translation Model)\n",
    "MODEL_NAME = \"facebook/nllb-200-distilled-600M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Function to translate English to Hindi\n",
    "def translate_sentence(text, src_lang=\"eng_Latn\", tgt_lang=\"hin_Deva\"):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs[\"forced_bos_token_id\"] = tokenizer.convert_tokens_to_ids(tgt_lang)\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        translated_tokens = model.generate(**inputs)\n",
    "    \n",
    "    return tokenizer.decode(translated_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "# Interactive Loop\n",
    "print(\"🔹 English-to-Hindi Translator (Type 'exit' to quit) 🔹\")\n",
    "while True:\n",
    "    text = input(\"\\nEnter English text: \")\n",
    "    if text.lower() == \"exit\":\n",
    "        print(\"👋 Exiting translator. Have a great day!\")\n",
    "        break\n",
    "    translation = translate_sentence(text)\n",
    "    print(f\"📝 Translated: {translation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb498ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRANSLATION EVALUATION RESULTS =====\n",
      "🔹 Average BLEU Score: 0.1238\n",
      "🔹 Average ROUGE-1 Score: 0.3669\n",
      "🔹 Average ROUGE-2 Score: 0.1680\n",
      "🔹 Average ROUGE-L Score: 0.3465\n",
      "🔹 Emotion Accuracy: 83.70%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge import Rouge\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load Data\n",
    "translated_data_path = \"nllb_translated_output.csv\"  # Updated file path\n",
    "data = pd.read_csv(translated_data_path).dropna(subset=[\"Original_Hindi\", \"Translated_Text\"])  # Ensure no missing values\n",
    "\n",
    "# Initialize Metrics\n",
    "rouge = Rouge()\n",
    "smoothie = SmoothingFunction().method4  # BLEU smoothing function\n",
    "\n",
    "# Function to Compute BLEU Score\n",
    "def compute_bleu(reference, hypothesis):\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str) or not reference or not hypothesis:\n",
    "        return 0.0  # Handle empty strings safely\n",
    "    \n",
    "    reference_tokens = [reference.split()]\n",
    "    hypothesis_tokens = hypothesis.split()\n",
    "    \n",
    "    return sentence_bleu(reference_tokens, hypothesis_tokens, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "# Function to Compute ROUGE Score\n",
    "def compute_rouge(reference, hypothesis):\n",
    "    if not isinstance(reference, str) or not isinstance(hypothesis, str) or not reference or not hypothesis:\n",
    "        return {\"rouge-1\": 0.0, \"rouge-2\": 0.0, \"rouge-l\": 0.0}\n",
    "    \n",
    "    scores = rouge.get_scores(hypothesis, reference)[0]  # Get ROUGE scores\n",
    "    return {\n",
    "        \"rouge-1\": scores[\"rouge-1\"][\"f\"],\n",
    "        \"rouge-2\": scores[\"rouge-2\"][\"f\"],\n",
    "        \"rouge-l\": scores[\"rouge-l\"][\"f\"]\n",
    "    }\n",
    "\n",
    "# Function to Evaluate Emotion Accuracy\n",
    "def compute_emotion_accuracy(data):\n",
    "    if \"Input_Emotion\" not in data.columns or \"Translated_Emotion\" not in data.columns:\n",
    "        print(\"⚠️ Emotion columns not found, skipping emotion accuracy calculation.\")\n",
    "        return None\n",
    "\n",
    "    actual_emotions = data[\"Input_Emotion\"].tolist()\n",
    "    translated_emotions = data[\"Translated_Emotion\"].tolist()\n",
    "    \n",
    "    # Filter out cases where the translated emotion is \"UNKNOWN\"\n",
    "    valid_indices = [i for i in range(len(actual_emotions)) if translated_emotions[i] != \"UNKNOWN\"]\n",
    "    \n",
    "    if not valid_indices:\n",
    "        return 0.0  # Avoid division by zero\n",
    "\n",
    "    actual_filtered = [actual_emotions[i] for i in valid_indices]\n",
    "    translated_filtered = [translated_emotions[i] for i in valid_indices]\n",
    "\n",
    "    return accuracy_score(actual_filtered, translated_filtered)\n",
    "\n",
    "# Compute Scores for Each Row\n",
    "bleu_scores = []\n",
    "rouge1_scores, rouge2_scores, rougel_scores = [], [], []\n",
    "\n",
    "for _, row in data.iterrows():\n",
    "    reference = row[\"Original_Hindi\"]\n",
    "    hypothesis = row[\"Translated_Text\"]\n",
    "\n",
    "    bleu_scores.append(compute_bleu(reference, hypothesis))\n",
    "    rouge_scores = compute_rouge(reference, hypothesis)\n",
    "    \n",
    "    rouge1_scores.append(rouge_scores[\"rouge-1\"])\n",
    "    rouge2_scores.append(rouge_scores[\"rouge-2\"])\n",
    "    rougel_scores.append(rouge_scores[\"rouge-l\"])\n",
    "\n",
    "# Compute Averages\n",
    "avg_bleu = np.mean(bleu_scores)\n",
    "avg_rouge1 = np.mean(rouge1_scores)\n",
    "avg_rouge2 = np.mean(rouge2_scores)\n",
    "avg_rougel = np.mean(rougel_scores)\n",
    "emotion_accuracy = compute_emotion_accuracy(data)\n",
    "\n",
    "# Display Evaluation Results\n",
    "print(\"\\n===== TRANSLATION EVALUATION RESULTS =====\")\n",
    "print(f\"🔹 Average BLEU Score: {avg_bleu:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-1 Score: {avg_rouge1:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-2 Score: {avg_rouge2:.4f}\")\n",
    "print(f\"🔹 Average ROUGE-L Score: {avg_rougel:.4f}\")\n",
    "\n",
    "if emotion_accuracy is not None:\n",
    "    print(f\"🔹 Emotion Accuracy: {emotion_accuracy:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
